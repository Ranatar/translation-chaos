import natural from 'natural'; // npm install natural
import embeddingService from './embeddings.js';

class AnalyticsService {
  constructor() {
    this.tokenizer = new natural.WordTokenizer();
    this.tfidf = new natural.TfIdf();
  }

  // ====== –î–ï–¢–ê–õ–¨–ù–´–ô –ê–ù–ê–õ–ò–ó –ö–ê–ñ–î–û–ì–û –®–ê–ì–ê ======
  async analyzeStep(currentText, previousText, fromLang, toLang, service) {
    // –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
    const currentTokens = this.tokenizer.tokenize(currentText.toLowerCase());
    const previousTokens = this.tokenizer.tokenize(previousText.toLowerCase());

    // –ü–æ—Ç–µ—Ä—è–Ω–Ω—ã–µ –∏ –Ω–æ–≤—ã–µ —Å–ª–æ–≤–∞
    const lost = previousTokens.filter(t => !currentTokens.includes(t));
    const gained = currentTokens.filter(t => !previousTokens.includes(t));
    const preserved = previousTokens.filter(t => currentTokens.includes(t));

    // –ò–∑–º–µ–Ω–µ–Ω–∏–µ –¥–ª–∏–Ω—ã
    const lengthChange = currentText.length - previousText.length;
    const lengthChangePercent = (lengthChange / previousText.length) * 100;

    // –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑
    const prevEmb = await embeddingService.getEmbedding(previousText);
    const currEmb = await embeddingService.getEmbedding(currentText);
    const localSimilarity = prevEmb && currEmb 
      ? embeddingService.cosineSimilarity(prevEmb, currEmb)
      : embeddingService.jaccardSimilarity(previousText, currentText);

    // –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ –∏–∑–º–µ–Ω–µ–Ω–∏—è
    const changeType = this.classifyChange(localSimilarity);

    // –í–æ–∑–º–æ–∂–Ω—ã–µ –ø—Ä–∏—á–∏–Ω—ã –¥—Ä–µ–π—Ñ–∞
    const reasons = this.analyzeDriftReasons(
      fromLang, 
      toLang, 
      lost, 
      gained, 
      localSimilarity
    );

    return {
      fromLang,
      toLang,
      service,
      previousText,
      currentText,
      localSimilarity,
      localDrift: 1 - localSimilarity,
      changeType,
      tokens: {
        lost,
        gained,
        preserved,
        lostCount: lost.length,
        gainedCount: gained.length,
        preservedCount: preserved.length
      },
      lengthChange,
      lengthChangePercent: Math.round(lengthChangePercent),
      reasons,
      confidence: this.calculateConfidence(service, localSimilarity)
    };
  }

  classifyChange(similarity) {
    if (similarity > 0.95) return { type: 'minimal', label: '–ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ', icon: '‚úì' };
    if (similarity > 0.85) return { type: 'stable', label: '–°—Ç–∞–±–∏–ª—å–Ω—ã–π —à–∞–≥', icon: '‚Üí' };
    if (similarity > 0.70) return { type: 'moderate', label: '–£–º–µ—Ä–µ–Ω–Ω—ã–π –¥—Ä–µ–π—Ñ', icon: '‚ö†Ô∏è' };
    if (similarity > 0.50) return { type: 'significant', label: '–ó–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–π —Å–¥–≤–∏–≥', icon: '‚ö°' };
    return { type: 'critical', label: '–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–π —Å–¥–≤–∏–≥', icon: 'üî•' };
  }

  analyzeDriftReasons(fromLang, toLang, lost, gained, similarity) {
    const reasons = [];

    // –†–µ–¥–∫–∞—è —è–∑—ã–∫–æ–≤–∞—è –ø–∞—Ä–∞
    const rarePairs = ['eu', 'ka', 'is', 'mt', 'cy', 'hy'];
    if (rarePairs.includes(fromLang) || rarePairs.includes(toLang)) {
      reasons.push({
        type: 'rare_pair',
        description: `–†–µ–¥–∫–∞—è —è–∑—ã–∫–æ–≤–∞—è –ø–∞—Ä–∞ ${fromLang}‚Üí${toLang}`,
        impact: 'high'
      });
    }

    // –ë–æ–ª—å—à–∞—è –ø–æ—Ç–µ—Ä—è —Å–ª–æ–≤
    if (lost.length > 3) {
      reasons.push({
        type: 'word_loss',
        description: `–ü–æ—Ç–µ—Ä—è–Ω–æ ${lost.length} —Å–ª–æ–≤: ${lost.slice(0, 3).join(', ')}...`,
        impact: 'medium'
      });
    }

    // –ù–∏–∑–∫–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ
    if (similarity < 0.7) {
      reasons.push({
        type: 'semantic_shift',
        description: '–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π —Å–¥–≤–∏–≥ - –∏–∑–º–µ–Ω–µ–Ω–∏–µ –æ–±—â–µ–≥–æ —Å–º—ã—Å–ª–∞',
        impact: 'high'
      });
    }

    // –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –Ω–æ–≤—ã—Ö –∫–æ–Ω—Ü–µ–ø—Ç–æ–≤
    if (gained.length > lost.length) {
      reasons.push({
        type: 'concept_expansion',
        description: '–ü–æ—è–≤–ª–µ–Ω–∏–µ –Ω–æ–≤—ã—Ö –∫–æ–Ω—Ü–µ–ø—Ç–æ–≤ –≤ –ø–µ—Ä–µ–≤–æ–¥–µ',
        impact: 'medium'
      });
    }

    return reasons;
  }

  calculateConfidence(service, similarity) {
    // –≠–≤—Ä–∏—Å—Ç–∏–∫–∞ –¥–æ–≤–µ—Ä–∏—è –∫ —Ä–∞–∑–Ω—ã–º —Å–µ—Ä–≤–∏—Å–∞–º
    const serviceConfidence = {
      'google': 0.85,
      'mymemory': 0.70,
      'libretranslate': 0.75
    };

    const baseConfidence = serviceConfidence[service] || 0.70;
    
    // –ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É–µ–º –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—Ö–æ–¥—Å—Ç–≤–∞
    const adjusted = baseConfidence * (0.5 + similarity * 0.5);
    
    return Math.round(adjusted * 100) / 100;
  }

  // ====== –°–ï–ú–ê–ù–¢–ò–ß–ï–°–ö–ò–ï –ö–õ–ê–°–¢–ï–†–´ ======
  async extractSemanticClusters(text) {
    // –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö –∫–æ–Ω—Ü–µ–ø—Ç–æ–≤ —á–µ—Ä–µ–∑ TF-IDF
    this.tfidf.addDocument(text);
    
    const terms = [];
    this.tfidf.listTerms(0).forEach(item => {
      if (item.tfidf > 0.1) {
        terms.push({ term: item.term, weight: item.tfidf });
      }
    });

    // –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–º –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º (—É–ø—Ä–æ—â—ë–Ω–Ω–æ)
    const clusters = this.categorizeTerms(terms);

    return clusters;
  }

  categorizeTerms(terms) {
    // –°–ª–æ–≤–∞—Ä–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–π (—É–ø—Ä–æ—â—ë–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)
    const categories = {
      movement: ['–ø–æ—à—ë–ª', '–∏–¥—Ç–∏', 'went', 'go', 'move', 'walk'],
      commerce: ['–º–∞–≥–∞–∑–∏–Ω', 'shop', 'store', '–∫—É–ø–∏—Ç—å', 'buy'],
      religion: ['—Ö—Ä–∞–º', 'temple', 'church', '—Å–≤—è—Ç–æ–π', 'sacred'],
      food: ['—Ö–ª–µ–±', 'bread', '–µ–¥–∞', 'food'],
      emotion: ['–ª—é–±–æ–≤—å', 'love', '—Ä–∞–¥–æ—Å—Ç—å', 'joy', '—Å—Ç—Ä–∞—Ö', 'fear']
    };

    const result = {};
    
    for (const term of terms) {
      for (const [category, keywords] of Object.entries(categories)) {
        if (keywords.some(kw => term.term.includes(kw) || kw.includes(term.term))) {
          if (!result[category]) result[category] = [];
          result[category].push(term.term);
        }
      }
    }

    return result;
  }

  // ====== –ò–ù–¢–ï–†–ï–°–ù–´–ï –ú–£–¢–ê–¶–ò–ò ======
  findInterestingMutations(results, analysis) {
    const mutations = [];

    for (let i = 1; i < results.length; i++) {
      const step = analysis[i - 1];
      
      // –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–π —Å–¥–≤–∏–≥
      if (step.localDrift && step.localDrift > 0.4) {
        mutations.push({
          type: 'critical_drift',
          step: i,
          language: results[i].language,
          description: `–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–π —Å–¥–≤–∏–≥ –Ω–∞ —à–∞–≥–µ ${i}: ${results[i - 1].text.substring(0, 50)} ‚Üí ${results[i].text.substring(0, 50)}`,
          drift: step.localDrift
        });
      }

      // –ù–µ–æ–∂–∏–¥–∞–Ω–Ω–æ–µ –∏—Å—á–µ–∑–Ω–æ–≤–µ–Ω–∏–µ —Å–ª–æ–≤–∞
      if (step.tokens && step.tokens.lostCount > 5) {
        mutations.push({
          type: 'word_disappearance',
          step: i,
          language: results[i].language,
          description: `–ú–∞—Å—Å–æ–≤–æ–µ –∏—Å—á–µ–∑–Ω–æ–≤–µ–Ω–∏–µ —Å–ª–æ–≤: ${step.tokens.lost.slice(0, 3).join(', ')}`,
          lostWords: step.tokens.lost
        });
      }

      // –ü–æ—è–≤–ª–µ–Ω–∏–µ –Ω–æ–≤–æ–≥–æ –∫–æ–Ω—Ü–µ–ø—Ç–∞
      if (step.reasons && step.reasons.some(r => r.type === 'concept_expansion')) {
        mutations.push({
          type: 'new_concept',
          step: i,
          language: results[i].language,
          description: `–ü–æ—è–≤–ª–µ–Ω–∏–µ –Ω–æ–≤—ã—Ö –∫–æ–Ω—Ü–µ–ø—Ç–æ–≤: ${step.tokens.gained.slice(0, 3).join(', ')}`,
          newWords: step.tokens.gained
        });
      }
    }

    return mutations;
  }
}

export default new AnalyticsService();